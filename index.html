<!DOCTYPE html>
<!-- Coding by CodingNepal || www.codingnepalweb.com -->
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- Boxicons CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
    <link href="https://unpkg.com/boxicons@2.1.4/css/boxicons.min.css" rel="stylesheet" />
    <title>Quality and Trust Solution Documentation</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <!-- navbar -->
    <nav class="navbar">
      <div class="logo_item">
        <i class="fa-solid fa-bars" id="sidebarOpen"></i>
        <!-- <img src="images/logo.png" alt=""></i> -->
        Quality and Trust Solution
      </div>

      <div class="search_bar">
        <!-- <input type="text" placeholder="Search" /> -->
      </div>

      <div class="navbar_content">
        <i class="bi bi-grid"></i>
        <i class="fa-solid fa-sun" id="darkLight"></i>
        <img src="images/profile.jpg" alt="" class="profile" />
      </div>
    </nav>

    <!-- sidebar -->
    <nav class="sidebar">
      <div class="menu_content">
        <ul class="menu_items">
          

          <div class="menu_title menu_dahsboard"></div>
          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-house"></i>
              </span>
              <span class="navlink">Home</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink qts" onClick="clickCalled('qts')">Quality & Trust Solution</a>
              <a href="#" class="nav_link sublink intro" onClick="clickCalled('intro')">Introduction</a>
              <a href="#" class="nav_link sublink busRel" onClick="clickCalled('busRel')">Business Relevance</a>
              <a href="#" class="nav_link sublink relUC" onClick="clickCalled('relUC')">Relevant Use cases</a>
              <a href="#" class="nav_link sublink clinicDefSev" onClick="clickCalled('clinicDefSev')">Clinical Defect Severity </a>
            </ul>
          </li>
          <!-- end -->

          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-screwdriver-wrench"></i>
              </span>
              <span class="navlink">Installation</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink preReq" onClick="clickCalled('preReq')">Prerequisite</a>
              <a href="#" class="nav_link sublink packIns" onClick="clickCalled('packIns')">Package Installation</a>
              <a href="#" class="nav_link sublink errRes" onClick="clickCalled('errRes')">Error Resolution</a>
            </ul>
          </li>
          <!-- end -->

          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-sliders"></i>
              </span>
              <span class="navlink">Configuration</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink llm" onClick="clickCalled('llm')">LLM Model Config</a>
              <a href="#" class="nav_link sublink dbConf" onClick="clickCalled('dbConf')">Database Config:</a>
              <a href="#" class="nav_link sublink setDb" onClick="clickCalled('setDb')">Setup database & use Lib</a>
              <a href="#" class="nav_link sublink mmc" onClick="clickCalled('mmc')">Metric/Metadata Config</a>
            </ul>
          </li>
          <!-- end -->

          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-handshake"></i>
              </span>
              <span class="navlink">How to use QT</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink qtLib" onClick="clickCalled('qtLib')">How to use QT Lib</a>
              <a href="#" class="nav_link sublink errResolution" onClick="clickCalled('errResolution')">Error Resolution</a>
              <a href="#" class="nav_link sublink fnq" onClick="clickCalled('fnq')">F&Q</a>
            </ul>
          </li>
          <!-- end -->
        </ul>

        <ul class="menu_items">
          <div class="menu_title menu_editor"></div>
          <!-- duplicate these li tag if you want to add or remove navlink only -->
          <li class="item">
            <a href="#" class="nav_link matrIntro" onClick="clickCalled('matrIntro')">
              <span class="navlink_icon">
                <i class="fa-solid fa-circle"></i>
              </span>
              <span class="navlink">Introduction</span>
            </a>
          </li>

<!-- Response_QA Start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-circle"></i>
              </span>
              <span class="navlink">Metrics for Q&A</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_exact" onClick="clickCalled('matrQA_exact')">Accuracy</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_complete" onClick="clickCalled('matrQA_complete')">Relevancy</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_hallucination" onClick="clickCalled('matrQA_hallucination')">Hallucination</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_syntax" onClick="clickCalled('matrQA_syntax')">Robustness</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_Latency" onClick="clickCalled('matrQA_Latency')">Efficiency</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_gender" onClick="clickCalled('matrQA_gender')">Bias</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_toxic" onClick="clickCalled('matrQA_toxic')">Toxicity</span>
            </ul>
          </li>
<!-- Response_QA End -->
<!-- Healthcare Specific Metrics  -->
          <li class="item">
            <a href="#" class="nav_link matrClinic_defect" onClick="clickCalled('matrClinic_defect')">
              <span class="navlink_icon">
                <i class="fa-solid fa-circle"></i>
              </span>
              <span class="navlink">Healthcare Specific Metrics</span>
            </a>
          </li>
<!-- Response_summarization Start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-circle"></i>
              </span>
              <span class="navlink">Metrics for Summarization</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrSum_rouge" onClick="clickCalled('matrSum_rouge')">Accuracy</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrSum_supert" onClick="clickCalled('matrSum_supert')">Relevancy</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrSum_hallucination" onClick="clickCalled('matrSum_hallucination')">Hallucination</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrSum_coherance" onClick="clickCalled('matrSum_coherance')">Robustness</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrSum_cost" onClick="clickCalled('matrSum_cost')">Efficiency</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrSum_gender" onClick="clickCalled('matrSum_gender')">Bias</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrSum_toxic" onClick="clickCalled('matrSum_toxic')">Toxicity</span>
            </ul>
          </li>
<!-- Response_summarization End -->

        </ul>
        <ul class="menu_items">
          <div class="menu_title menu_model_benchmarking"></div>
        </ul>
        <ul class="menu_items">
          <div class="menu_title menu_methods_summarization"></div>
        </ul>
        <ul class="menu_items">
          <div class="menu_title menu_ref"></div>
        </ul>

      </div>
    </nav>
    <div id="maincontent">
      <div id="default">
        <h2 class="pageTitle">Quality & Trust Solution</h2>
CitiusTech's Healthcare GenAI Quality & Trust Solution is a software-based framework to design, develop, integrate, and monitor quality and trust of GenAI applications in healthcare to drive enterprise adoption and scaling.<br><br>QT solution is an evaluator to assess the performance and effectiveness of applications utilizing LLMs for the tasks: question answering, summarization, classification and NER & entity extraction. Also evaluates the search and retrieval part of a RAG application. Perform reference free evaluations of parts of the RAG pipeline. These metrics can be a part of CI/CD pipelines for improving prompts for entity extraction, response quality, search quality and application quality during development.
      </div>

      <div id="qts">
        <h2 class="pageTitle">Quality & Trust Solution</h2>
CitiusTech's Healthcare GenAI Quality & Trust Solution is a software-based framework to design, develop, integrate, and monitor quality and trust of GenAI applications in healthcare to drive enterprise adoption and scaling.<br><br>QT solution is an evaluator to assess the performance and effectiveness of applications utilizing LLMs for the tasks: question answering, summarization, classification and NER & entity extraction. Also evaluates the search and retrieval part of a RAG application. Perform reference free evaluations of parts of the RAG pipeline. These metrics can be a part of CI/CD pipelines for improving prompts for entity extraction, response quality, search quality and application quality during development.
      </div>

      <div id="intro">
        <h2 class="pageTitle">Introduction</h2>
<b>Citiustech Healthcare GENAI Quality & Trust (Q&T) Framework</b> is a software-based solution designed to ensure the quality, reliability, and trustworthiness of Generative AI (GENAI) applications in the healthcare industry. As organizations increasingly adopt AI-driven solutions, particularly Large Language Models (LLMs), it becomes crucial to monitor and assess their performance. The Q&T Framework is built to evaluate applications that leverage LLMs for critical tasks like question answering, summarization, classification, Named Entity Recognition (NER), and entity extraction.
<br><br> framework is pivotal for organizations aiming to implement and scale GENAI applications within healthcare ecosystems. By integrating the Q&T Framework into the Continuous Integration/Continuous Deployment (CI/CD) pipeline, healthcare organizations can monitor, improve, and optimize the quality of AI-generated outputs, ensuring regulatory compliance, enhanced accuracy, and ultimately better patient outcomes.        
      </div>

      <div id="busRel">
        <h2 class="pageTitle">Business Relevance</h2>
Generative AI is revolutionizing healthcare, from automating administrative tasks to assisting with clinical decision-making. However, its successful enterprise-wide adoption hinges on ensuring the quality, accuracy, and trustworthiness of AI-driven applications. The Citiustech Healthcare GENAI Quality & Trust Framework addresses this need by providing a structured, evaluative approach that organizations can use to: <br>
• <b>Mitigate Risks:</b> Ensure that AI models are accurate, transparent, and ethical, thereby mitigating risks associated with incorrect or biased outputs in healthcare scenarios.
• <b>Enhance Trust:</b> Build trust among healthcare providers and patients by ensuring that AI-driven decisions are reliable and aligned with healthcare standards and regulations.
• <b>Optimize Performance:</b> The framework evaluates model effectiveness during development and post-deployment, improving performance in real time by identifying areas for prompt enhancement and error correction.
• <b>Accelerate Scaling:</b> By integrating the framework into CI/CD pipelines, organizations can streamline the development and deployment process, facilitating large-scale adoption of GENAI applications across multiple healthcare use cases.
<br>With a focus on tasks like question answering, summarization, classification, NER, entity extraction, and RAG (Retrieval-Augmented Generation) applications, the Q&T Framework supports ongoing AI model refinement, making it a vital tool for scaling AI innovations in healthcare.
      </div>

      <div id="relUC">
        <h2 class="pageTitle">Relevant Use Cases</h2>
<i>1. Question Answering for Clinical Decision Support</i>
<b>• Challenge:</b> Healthcare professionals often rely on AI models to provide fast, accurate answers to complex medical questions based on large datasets of clinical knowledge. Ensuring the quality and reliability of these answers is critical for patient safety.
<b>• Solution:</b> The Q&T Framework evaluates the effectiveness of LLMs in clinical question-answering tasks, ensuring that responses are accurate, relevant, and free from bias. Continuous evaluation helps refine model responses over time, improving clinical decision-making.
<br><i>2. Summarization of Medical Records</i>
<b>• Challenge:</b> Healthcare providers require concise, accurate summaries of extensive patient medical records, which are often generated by AI systems. Errors in summarization can lead to missed diagnoses or incorrect treatment plans.
<b>• Solution:</b> The Q&T Framework assesses the quality of summarizations produced by GENAI models, ensuring the summaries are clear, comprehensive, and medically accurate. The evaluation results can be used to fine-tune summarization models to better meet clinical requirements.
<br><i>3. Classification of Medical Data</i>
<b>• Challenge:</b> In applications like diagnostic tools and medical billing, AI models must accurately classify medical data (e.g., images, clinical notes) into predefined categories. Misclassification can result in incorrect diagnoses or billing errors.
<b>• Solution:</b> The Q&T Framework evaluates the precision, recall, and overall accuracy of classification models, allowing healthcare organizations to monitor and improve model performance, ensuring data is classified correctly.
<br><i>4. Named Entity Recognition (NER) and Entity Extraction from Clinical Texts</i>
<b>• Challenge:</b> AI systems in healthcare must accurately identify medical entities (e.g., diseases, drugs, symptoms) from unstructured clinical notes or research papers. Errors in NER or entity extraction can lead to incomplete patient data or incorrect analytics.
<b>• Solution:</b> The Q&T Framework assesses the quality of NER and entity extraction tasks, ensuring that models accurately recognize and extract relevant entities from text. By integrating this into the CI/CD pipeline, developers can continuously improve the performance of NER models.
<br><i>5. Search and Retrieval for RAG (Retrieval-Augmented Generation) Applications</i>
<b>• Challenge:</b> In RAG applications, LLMs rely on search and retrieval functions to generate relevant and accurate responses. Poor search results can lead to incomplete or incorrect AI outputs.
<b>• Solution:</b> The Q&T Framework evaluates the effectiveness of the search and retrieval process in RAG applications. By continuously monitoring this aspect, the framework ensures that LLMs retrieve the most relevant information, improving overall response quality in GENAI applications.

How the Q&T Framework Supports Development and Scaling
<b>• Reference-Free Evaluation:</b> The Q&T Framework can perform reference-free evaluations of specific components in the RAG pipeline. This means that the framework can assess model outputs without needing predefined correct answers, making it versatile in evaluating complex, open-ended tasks such as summarization or question answering.
<b>• CI/CD Pipeline Integration:</b> By incorporating evaluation metrics into the CI/CD pipelines, developers can automatically monitor and improve the quality of entity extraction, response quality, and search performance as new iterations of the AI model are deployed. This real-time feedback mechanism ensures that healthcare AI applications continuously evolve and improve in terms of accuracy, relevance, and trustworthiness.
<b>• Metrics for Trust and Quality:</b> The framework provides actionable metrics that help developers assess the quality and trust of the AI applications they are building. These metrics can drive prompt engineering, improving model performance at every stage of the AI lifecycle.
      </div>

      <div id="clinicDefSev">
        <h2 class="pageTitle">Clinical Defect Severity </h2>
(LLM Based Evaluator) 

Clinical Hallucination Severity measures the severity of defects in the summary. It aims to rate the harm caused by the issues in the LLM generated summary. 

<b>Required Arguments:</b> (your dataset must contain these fields) 

<b>document:</b> The source document that contains the information that should be summarized. 

<b>reference summary:</b> SME generated reference summary. 

<b>response/ summary:</b> The LLM generated summary of the source document. 


<b>Calculation: </b>

Fine-tune the prompt on reference summary or source document.  

Compare source document with the LLM generated summary. 

Assign score based on severity of defect scale below: 

Low impact: The defect is so immaterial that it is unlikely to be noticed.  

Mild impact: The defect could potentially lead to misunderstandings or miscommunication but is still just annoying rather than actively harmful.  

Moderate impact: The defect poses a risk of moderate harm if not corrected, for example omitting some detail about a symptom relevant to the chief complaint.  

Major impact: The defect poses a significant risk of causing incorrect treatment or diagnosis, demanding immediate correction.  

Critical impact: The defect could lead to serious adverse patient outcomes without correction, such as incorrect surgery or medication. 

  

<b>Output: </b>

A severity score on the defect severity scale. 
      </div>

      <div id="preReq">
        <h2 class="pageTitle">Prerequisite</h2>
• Python Version 3.10.* to 3.11.*

• MySQL Workbench

• Postgres

• Environment variables to be set and configured. Refer to Configurations for further steps.
      </div>

      <div id="packIns">
        <h2 class="pageTitle">Package Installation</h2>
<b>Steps:-</b>

<b>Step 1:</b> Get access to Quality & Trust gitlab repository. Download Q&T package wheel file from https://git/generativeai/genai-trust-framework/-/tree/QT_backend/dist

<b>Step 2:</b> For Package Installation, create a new virtual environment and activate it or install the package in existing application environment.

<b>Step 3:</b> To install wheel file, execute below command -
<b>pip install qualitytrust-1.0-py3-none-any.whl</b>

All the libraries will be installed with dependencies.

<b>Note -</b> if you face any issue while installation, please refer to error resolution page (link).
      </div>

      <div id="errRes">
        <h2 class="pageTitle">Error Resolution</h2>
Error : Visual Build Tool error
Solution : Download and install visual C++ build tool

Error : Python package dependency errors
Solution : Downgrade/upgrade common packages.        
      </div>

      <div id="llm">
        <h2 class="pageTitle">LLM Model Config</h2>
<b>Environment Variables</b>
There are two types of metrics in the Q&T solution package: reference based and reference free metrics. Reference based metrics need a ground truth against which to evaluate the application response. Reference free metrics in the Q&T solution are evaluated using LLMs. These metrics align with human expectations and can be computed using any LLM. An LLM is a Judge for evaluation of these non-ground truth based/ reference free metrics. The user can switch between different models or even open source LLMs by providing the model details. Following are the methods to set up / configure the models and to setup environment variables.

<b>For Azure OpenAI model:</b>
Run the following commands to configure your environment to use AzureOpenAI models for all LLM-based metrics. To use AzureOpenAI models for evaluation, supply the model details as below:
• os.environ["MODEL_TYPE"] = "azure"
• os.environ["AZURE_OPENAI_API_KEY"] = "1213jaj...." 
• os.environ["Azure_OPENAI_VERSION"] = ""
• os.environ["AZURE_API_BASE_URL"] = ""
• os.environ["AZURE_MODEL_DEPLOYMENT_NAME"] ="davinci"

<b>For OpenAI models :</b> 
• os.environ["MODEL_TYPE"] = "openai"
• os.environ["OPENAI_API_KEY"] = "sk-..."
• os.environ["OPENAI_MODEL_NAME"] = ""

<b>For Anthropic Claude models:</b>
• os.environ["MODEL_TYPE"] = "claude"
• os.environ["ANTHROPIC_API_KEY"] = "sk-..."
• os.environ["CLAUDE_MODEL_NAME"] = ""

<b>For Mistral models:</b>
• os.environ["MODEL_TYPE"] = "mistral"
• os.environ["MISTRAL_API_KEY"] = "sk-..."
• os.environ["MISTRAL_MODEL_NAME"] = ""

<b>For Ollama models:</b> 
  Run the following commands to configure your environment to use an open source model for all LLM-based metrics. Quantized models can be used on the CPU without high latency/ without increasing computational costs. Some models that you can try are: Llama 3.1, MS phi3.5 mini etc
• os.environ["MODEL_TYPE"] = "ollama"
• os.environ["OLLAMA_MODEL_NAME"] = ""

<b>For AWS Bedrock models:</b>
• os.environ["MODEL_TYPE"] = "bedrock"
• os.environ["AWS_ACCESS_KEY_ID"] = ""
• os.environ["AWS_SECRET_ACCESS_KEY"] = ""
• os.environ["AWS_REGION_NAME"] = ""
• os.environ["BEDROCK_MODEL_NAME"] = ""
      </div>

      <div id="dbConf">
        <h2 class="pageTitle">Database Config</h2>
To integrate Q&T with any application, we need database to store evaluation scores that can be visualized in dashboard. Hence, we need to set environment variables to configure

<b>For mysql database:</b>
• os.environ["DB_TYPE"] = "mysql"
• os.environ["MYSQL_USER"] = ""
• os.environ["MYSQL_PASSWORD"] = ""
• os.environ["MYSQL_HOST"] = ""
• os.environ["MYSQL_PORT"] = "3306"
• os.environ["MYSQL_DB_NAME"] = ""

<b>For postgres database:</b>
• os.environ["DB_TYPE"] = "postgres"
• os.environ["POSTGRES_USER"] = ""
• os.environ["POSTGRES_PASSWORD"] = ""
• os.environ["POSTGRES_HOST"] = ""
• os.environ["POSTGRES_PORT"] = "5432"
• os.environ["POSTGRES_NAME"] = ""

<b>For mssql database:</b>
• os.environ["DB_TYPE"] = "mssql"
• os.environ["MSSQL_USER"] = ""
• os.environ["MSSQL_PASSWORD"] = ""
• os.environ["MSSQL_HOST"] = ""
• os.environ["MSSQL_PORT"] = "1433"
• os.environ["MSSQL_NAME"] = ""

<b>For oracle database :</b>
• os.environ["DB_TYPE"] = "oracle"
• os.environ["ORACLE_USER"] = ""
• os.environ["ORACLE_PASSWORD"] = ""
• os.environ["ORACLE_HOST"] = ""
• os.environ["ORACLE_PORT"] = "1521"
• os.environ["ORACLE_NAME"] = ""


<b>For databricks database :</b>
• os.environ["DB_TYPE"] = "delta_table"
• os.environ["DATABRICKS_HTTP_PATH"] = ""
• os.environ["DATABRICKS_ACCESS_TOKEN"] = ""
• os.environ["DATABRICKS_HOST"] = ""
• os.environ["DATABRICKS_CATALOG"] = ""
• os.environ["DATABRICKS_SCHEMA"] = ""

If none of the above databases is used, by default sqlite database is used and quality_trust.db is created.
      </div>

      <div id="setDb">
        <h2 class="pageTitle">How to setup database and use library?</h2>
<b>How to setup database</b>

• Install any of the database tools like postgres, MySQL workbench/server, MS SQL, Oracle etc. 

• Setup credentials in the tool and accordingly change the database configuration (link of db config)

• Create a database in database tool and update database name in database configuration (link)

• To create schema, follow these steps - 

<b>Steps to create schema</b>
Run the following python commands -  

<b>#Import below library from qualitytrust</b>
from qualitytrust import model

<b>#Execute the following command</b>
model.create_schema()


<b>Insert metadata in database</b>
With this metadata following tables will be loaded -
applications
metric_type
metrics
metric_mapping

Format of the schema which is provided explicitly should be as below –

- application: name_of_the_application
app_description: GenAI tool which assist clinical reviewers in decision support through infornation retrieval, extraction and accessing medical necessity of requested services
mapping:
- metric_type: Application
metrics:
- metric_class: A.F.T.R
description: ""
task: All
metric_name: A.F.T.R
soft_threshold: 0.7
hard_threshold: 0.4

This data should be in metric_config.yml file as per the requirements.

<b>Logging metadata in database</b>
Use the following commands to log metadata into the database using qualitytrust package

<b>#Import log metadata library </b>
from app.qualitytrust.suite.log_metadata import LogMetadata

<b>#Run the python commands</b>
log_metadata = LogMetadata()
log_metadata.log(metadata_path="app/qualitytrust/config/metric_config.yml")

      </div>

      <div id="mmc">
        <h2 class="pageTitle">Metric / Metadata Config</h2>
      </div>

      <div id="qtLib">
        <h2 class="pageTitle">How to use QualityTrust library ?</h2>
Here is an example to showcase how you can use the library to evaluate various metrics such as Factual Accuracy, Response Relevance, Response Completeness, Hallucination Degree, Context Disregard, Language Critique, Gender and Racial Bias, Honest Score, Toxic Opinions. 

<b>We have a dict data containing query, response and context -</b>
data = [{
"question": "What is the size of hernia sac?",
"response": "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions.",
"context": """ Gross Description Received in
formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous material"""
},{
"question": "What are the treatments given to john Brown?",
"context": """Repair of recurrent right inguinal hernia. HISTORY AND INDICATIONS FOR ADMISSION: Mr. Brown is a 54-year-old white male who presented with pain to Dr. Jeff Moore. He had a hernia repair, on the right, in the past, and this was recurrent. He was scheduled for surgery. HOSPITAL COURSE: The patient was admitted on 11/12/1999 and underwent surgery, and did fine. He was transferred to the floor. On 11/13/99 he is alert, awake, afebrile, taking a regular diet. Having bowel movements, and passing his urine normally. His incision is clean and dry. He is discharged home in satisfactory condition with Lortab PRN for pain. He is to follow up with his primary care physician, Dr. Moore, on Monday. D: 11/13/1999 T: 11/16/1999 wms cc: Jeff T. Moore, M.D. Tom W. Smith, M.D. Community General Hospital Anytown, USA HISTORY AND
PHYSICAL Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore, M.D. Consulting Physician Adm Date: 11/12/1999 DOB: 09/10/44 Page 1 of 2 REASON FOR ADMISSION: This is a 54 year old male, admitted here for repair of right inguinal hernia. HISTORY OF PRESENT ILLNESS: The patient has
had his hernia repaired in the past, elsewhere. Over the past number of months, he has seen this hernia come back and recur, and become larger. It causes discomfort. He is admitted for repair of a right inguinal hernia. PAST MEDICAL HISTORY: Denies. MEDICATIONS: None. PAST SURGICAL HISTORY: Hernia surgery on the right in the past. The patient also has had a left inguinal hernia repair in the past""", 
"response": "John Brown underwent surgery for a recurrent hernia repair. It is not specified what other treatments, if any, were given to him."
}]

To evaluate metrics import the following library from qualitytrust package
from app.qualitytrust.bench import Evaluate

<b>Score can be calculated using - </b>
score = Evaluate.evaluate_dataset(dataset=data,metrics=["Factual Accuracy","Response Relevance","Response Completeness","Hallucination Degree","Context Disregard", "Language Critique","Gender and Racial Bias","Honest Score","Toxic Opinions"]) 
      </div>

      <div id="matrIntro">
        <h2 class="pageTitle">Introduction</h2>
<img src="images/matrIntro.png">
      </div>

<!-- Accuracy -->
      <div id="matrQA_exact">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_exact')">Exact Match</button>
          <button class="button button2" onClick="clickCalled('matrQA_rouge')">ROUGE Score</button>
          <button class="button button2" onClick="clickCalled('matrQA_factual')">Factual Accuracy</button>
        </div>
(This is a Reference Based Evaluator)

The Exact Match metric measures how often the generated response (ex: an answer or a summary) exactly matches the reference (or ground truth answer). Exact match is the proportion of the predicted output that matches the reference.

<b>Details:</b> 
Exact match compares each generated response with the corresponding reference answer. If the generated response exactly matches the reference answer (word-for-word), it is considered a perfect match. 
 
<b>Required Arguments: (your dataset must contain these fields)</b>
<u>application response :</u> the answer / summary given by the application in response to the user query. 
<u>reference:</u> the ground truth answer for the user query.

<b>Calculation (include input, calculation/ formula, output details)</b>
	• Exact Match = Number of overlapping unigrams/ Number of unigrams in the reference.
	• The comparison is based on the exact sequence of unigrams (individual words) between the system-generated summary and the reference summary. Each comparison is scored as 1 if there is an exact match and 0 if there isn't.
	
<b>Output: </b>
	 A number between 0 and 1. 
<!-- How to use it?
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
 
response = """The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."""

reference = """Gross Description Received in formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous
material"""

eval_score = eval_obj.evaluate(response=response,reference=reference,metric="Exact Match")
print(eval_score) -->
<b>Limitations:</b>
	• It is a strict metric, meaning any minor difference (ex: punctuation, capitalization) will result in a score of 0.
	• Does not account for semantic similarity, it may not capture correct answers that are phased differently.

<!-- <b>Source:</b> <a href="https://huggingface.co/spaces/evaluate-metric/exact_match" target="_blank">Exact Match - a Hugging Face Space by evaluate-metric</a> -->

      </div>

      <div id="matrQA_rouge">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_exact')">Exact Match</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_rouge')">ROUGE Score</button>
          <button class="button button2" onClick="clickCalled('matrQA_factual')">Factual Accuracy</button>
        </div>
(This is a Reference Based Evaluator) 

(ROUGE-L is currently implemented in the QT package)

ROUGE score measures the overlap of words or phrases (n-grams) between the generated answer and reference answer.

<b>Details:</b> 
Recall-Oriented Understudy for Gisting Evaluation. It is used to assess the quality of automatic summarization systems. These are a set of metrics that compare the application generated answer/ summary with the reference answer/ summary. ROUGE is case insensitive. 
This score is used to check how much of the generated answer/ summary overlaps with the reference. 

<b>Types of Rouge score: </b>
• ROUGE-1 : unigram (1-gram) based scoring, I.e. measures that overlap of individual words. 
• ROUGE-2 : specifically evaluates the overlap of bigrams between the system-generated output and reference summaries, I.e. measures the overlap of pairs of words. 
• ROUGE-L : Longest common subsequence based scoring, I.e. measures the longest sequence of matching words. Ignores newlines and computes LSC for the entire text. 
• ROUGELsum: splits text using "\n". This is a variant of the ROUGE-L metric. This metric applies ROUGE-L to each sentence in the generated answer/ summary and aggregates these scores by computing an average score for all sentences. Suitable for extractive summarization tasks. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<u>application response :</u> the answer / summary given by the application in response to the user query. 
<u>reference:</u> the ground truth answer for the user query.

<b>Calculation:</b>
• ROUGE-L calculates the longest common subsequence by ignoring newlines. 
• Compares the text in the generated answer/ summary with the reference. 

<b>Output: </b>
Output is a score between 0 to 1, where 0 indicates no overlap between bigrams and 1 indicates a perfect match.
<!-- <b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()

response = """The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."""

reference = """Gross Description Received in formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous material"""

eval_score = eval_obj.evaluate(response=response,reference=reference,metric="RougeL")

print(eval_score) -->
<b>Limitations:</b>
• ROUGE score doesn't capture semantic meaning.
• May not handle paraphrasing or synonym usage well. 

<!-- <b>Source:</b>
• <a href="https://huggingface.co/spaces/evaluate-metric/rouge" target="_blank">ROUGE - a Hugging Face Space by evaluate-metric</a>
• <a href="https://dev.to/aws-builders/mastering-rouge-matrix-your-guide-to-large-language-model-evaluation-for-summarization-with-examples-jjg" target="_blank">Mastering ROUGE Matrix</a> -->
        
      </div>

      <div id="matrQA_factual">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_exact')">Exact Match</button>
          <button class="button button2" onClick="clickCalled('matrQA_rouge')">ROUGE Score</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_factual')">Factual Accuracy</button>
        </div>
(This is an LLM Based Evaluator)

Checks whether the response generated is factually correct and grounded by the provided context.

<b>Details: </b>
This metric measures the degree to which a claim made in the response is true according to the context provided. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<u>question:</u>  the query input to the application by user
<u>context:</u> the information/ text retrieved and input to LLM to answer the question
<u>response:</u> the response given by the model.

<b>Calculation:</b>
• Split the response to facts. The response is divided into different arguments, each stating a fact. Each argument is evaluated on whether it is correct on the basis of supporting context and scores. 
• Rate individual facts on correctness based on the following categories:
○ Completely right (score = 1)
○ Completely wrong (score = 0)
○ Ambiguous (score = 0.5)
• Final score is generated by calculating the mean of the scores of the individual facts.

<b>Output: </b>
A score between 0 and 1. 

<b>Algorithm Elaborated:</b>
<img src="images/matrQAfactual.png"> <br>
<!-- <b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric() 
query="What is the size of hernia sac"

response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

context = """Gross Description Received in
formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest
dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous
material"""

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Factual Accuracy")
print(eval_score)

<b>Sources:</b>
• <a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy" target="_blank">Factual Accuracy - UpTrain</a>
• <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context_awareness/factual_accuracy.ipynb" target="_blank">Factual Accuracy - Github</a>         -->
      </div>
<!-- Relevancy -->
      <div id="matrQA_complete">
        <h2 class="pageTitle">Relevancy</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_complete')">Response Completeness</button>
          <button class="button button2" onClick="clickCalled('matrQA_response')">Response Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_text')">Text Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_answer')">Answer Relevancy</button>
        </div>
(This is an LLM Based Evaluator)

Grades whether the response has answered all the aspects of the question specified.

<b>Details:</b> 
This score measures if the generated response has adequately answered all aspects to the user query asked. This ensures that the model is not generating incomplete responses. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question:</b>  the query input to the application by user
<b>response:</b> the response given by the model.

<b>Calculation:</b>
• Response completeness is calculated by determining which of the three cases apply to data. 
  ○ The generated answer does not answer the question, 
  ○ partially answers or 
  ○ adequately answers the given question. 
• If no aspect is answered, score is 0, if some are answered, score is 0.5, if all aspects of question are answered, score is 1.

<b>Output: </b>
Response is a score = 0, 0.5 or 1 for each query. A higher response completeness score indicates that the response has answered all aspects of the user's questions. 

<!-- <b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()

query="What is the size of hernia sac"

response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metrics="Response Completeness")
print(eval_score)


<b>Sources:</b>
• <a href="https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness" target="_blank">Completeness - UpTrain</a>
• <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/response_quality/completeness.ipynb" target="_blank">uptrain-ai/uptrain (github.com) </a> -->

      </div>

      <div id="matrQA_response">
        <h2 class="pageTitle">Relevancy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_complete')">Response Completeness</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_response')">Response Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_text')">Text Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_answer')">Answer Relevancy</button>
        </div>
(This is an LLM Based Evaluator)

Measures how relevant the generated response is to the question specified. It is a measure of how well the response addresses the question asked and if it contains any additional information irrelevant to the question asked. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question:</b>  the query input to the application by user
<b>response:</b> the response given by the model.

<b>Calculation:</b>
• The response is checked for additional irrelevant information, for staying focused on the question, and answering all aspects of the user question. Response is checked for its relevancy to the question.
• The LLM evaluates the response and scores it w.r.t different aspects of the user query being answered, and presence of irrelevant information.

<b>Output: </b>
A score between 0 and 1. A higher response relevance score reflects that the generated response is relevant to the question asked. 

<!-- <b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()

query="What is the size of hernia sac"

response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

context = """Gross Description Received in formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous material"""

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metrics="Response Relevance")
print(eval_score)

<b>Sources:</b>
• <a href="https://docs.uptrain.ai/predefined-evaluations/response-quality/response-relevance" target="_blank">Response Relevance - UpTrain</a>
• <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/response_quality/relevance.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a>   -->
      </div>

      <div id="matrQA_text">
        <h2 class="pageTitle">Relevancy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_complete')">Response Completeness</button>
          <button class="button button2" onClick="clickCalled('matrQA_response')">Response Relevance</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_text')">Text Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_answer')">Answer Relevancy</button>
        </div>
<!-- This metric has not been implemented in QT package due to dependency issues. -->
<b>Definition:</b> Evaluates relevance between prompts and responses by computing similarity scores between embeddings generated from prompts and responses.

<b>Details:</b> An objective measure of the similarity between different texts. It serves multiple use cases, including assessing the quality and appropriateness of LLM outputs.

<b>Library:</b>  Langkit

<b>Calculation:</b> Similarity score (cosine similarity) computed using the input_output module in langkit. 

<b>Inputs:</b> Input question and Application response

<b>Output:</b> The similarity score is computed by calculating the cosine similarity between embeddings generated from both prompt and response.
The embeddings are generated using the hugginface's model sentence-transformers/all-MiniLM-L6-v2.

<!-- Example: -->
        
      </div>

      <div id="matrQA_answer">
        <h2 class="pageTitle">Relevancy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_complete')">Response Completeness</button>
          <button class="button button2" onClick="clickCalled('matrQA_response')">Response Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_text')">Text Relevance</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_answer')">Answer Relevancy</button>
        </div>
<!-- This metric has not been implemented in QT package. -->
(This is an LLM Based Evaluator)

Evaluates how relevant the actual output of the application is compared to provided input.

<b>Details: </b>
The answer relevancy metric measures the quality of the RAG pipeline's generator by evaluating how relevant the actual output of your LLM application is compared to the provided input/ user query.

<b>Required Arguments:</b> (your dataset must contain these fields)
input/ user query:  the query input to the application by user
response: the response given by the model.

<b>Calculation:</b>
• Answer Relevancy = Number of Relevant Statements / Total Number of Statements
• The Answer Relevancy metric uses an LLM to extract all statements made in the LLM application response. 
• Then, the same LLM is used to classify whether each statement is relevant to the user query.

<b>Output: </b>
A higher answer relevancy score indicates that the response is relevant to the user's question.

<!-- <b>How to use it?</b>
add code here

<b>Sources:</b>
• <a href="https://docs.confident-ai.com/docs/metrics-answer-relevancy" target="_blank">Answer Relevancy | DeepEval</a>
• <a href="https://docs.confident-ai.com/docs/guides-rag-evaluation" target="_blank">RAG Evaluation | DeepEval</a> -->
      </div>
<!-- Hallucination -->
      <div id="matrQA_hallucination">
        <h2 class="pageTitle">Hallucination</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_hallucination')">Hallucination Degree</button>
          <button class="button button2" onClick="clickCalled('matrQA_validity')">Context Validity</button>
          <button class="button button2" onClick="clickCalled('matrQA_disregard')">Context Disregard</button>
        </div>
(This is an LLM Based Evaluator)

Grades how concise the generated response is or if it has any additional irrelevant information for the question asked.

<b>Details: </b>
This score measures whether the generated response contains any additional information irrelevant for the question asked. Response conciseness refers to the quality of a generated response in terms of being clear, brief, and to the point. A concise response effectively conveys the necessary information without unnecessary elaboration or verbosity. It focuses on addressing the core of the question or query in a straightforward manner.

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question:</b>  the query asked by user
<b>response:</b> the response given by the application

<b>Calculation:</b>
  • Response conciseness is calculated by determining which of the three cases apply for given task data:
    ○ The generated answer has a lot of additional irrelevant information
    ○ The generated answer has little additional irrelevant information, or 
    ○ The generated answer has no additional irrelevant information. 
  • Hallucination Degree = 1- Response Conciseness

<b>Output: </b>
  • Hallucination Degree equal to 0 indicates zero hallucination in the application response, that is, the application response has no additional irrelevant information. 
  • Hallucination Degree equal to 1 indicates that there is low response conciseness, that is, the application response has lot of additional irrelevant information. 

<!-- <b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()

Query = "In the case of John Brown, who underwent surgery for a recurrent right inguinal hernia, what were the findings in the pathology report regarding the soft tissue in the inguinal region?"

response = "The pathology report found that the soft tissue in the inguinal region contained a hernia sac containing hemorrhage and areas of fibrosis, but was negative for malignancy"

context = """inguinal hernia. Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore, M.D. Consulting Physician 1 Adm Date: 11/12/1999 DOB: 09/10/44 - 
POSTOPERATIVE DIAGNOSIS: Recurrent right inguinal hernia. PROCEDURE PERFORMED: Repair of recurrent right inguinal hernia, resection of lipoma of the | GENITALIA: Normal male. RECTAL: Negative. Prostate 1+. EXTREMITIES: Symmetric. IMPRESSION: 1. Right inguinal hernia recurrent. Community General
Hospital Anytown, USA HISTORY AND PHYSICAL Page 2 of 2 Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore, M.D. - Consulting | material. No ulceration, pigmentation or nodular abnormalities can be grossly identified. Representative portions submitted in one cassette. Diagnosis Soft tissue
inguinal region: Hernia sac containing hemorrhage and areas of fibrosis, negative for malignancy. Sally Johnson, M.D. Pathologist Dally Johnson Name: - 0.0.Bi | INDICATIONS FOR ADMISSION: Mr. Brown is a 54-year-old white male who presented with pain to Dr. Jeff Moore. He had a hernia repair, on the right, in the
past, and this was recurrent. He was scheduled for surgery. HOSPITAL COURSE: The patient was admitted on 11/12/1999 and underwent surgery, and did fine. | discomfort. He is admitted for repair of a right inguinal hernia. PAST MEDICAL HISTORY: Denies. MEDICATIONS: None. PAST SURGICAL HISTORY: Hernia
surgery on the right in the past. The patient also has had a left inguinal hernia repair in the past. EXAMINATION VITAL SIGNS: Blood pressure 140/90. | INGUINAL HERNIA Specimen Submitted HERNIA SAC Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore, M.D. Consulting Physician 1
Adm Date: 11/12/1999 DOB: 09/10/44 Ordering Physician: Jeff T. Moore Pathologist: Sally Johnson, M.D. Location: 3W 0328 P Gross Description Received in | of 2 REASON FOR ADMISSION: This is a 54 year old male, admitted here for repair of right inguinal hernia. HISTORY OF PRESENT ILLNESS: The patient has
had his hernia repaired in the past, elsewhere. Over the past number of months, he has seen this hernia come back and recur, and become larger. It causes | Physician: Jeff T. Moore, M.D. Consulting Physician Adm Date: 11/12/1999 DOB: 09/10/44 DISCHARGE SUMMARY Page 1 of 1 ADMITTING DIAGNOSIS: 1.
Recurrent right inguinal hernia. 1. DISCHARGE DIAGNOSIS: Same. PROCEDURES PERFORMED: 1. Repair of recurrent right inguinal hernia. HISTORY AND"""

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Hallucination Degree")

print(eval_score)

<b>Sources:</b>
• <a href="https://docs.uptrain.ai/predefined-evaluations/response-quality/response-conciseness" target="_blank">Response Conciseness - UpTrain</a>
• <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/response_quality/conciseness.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a>
• <a href="https://blog.uptrain.ai/revealing-the-hidden-truths-the-negative-impacts-of-hallucinations-in-large-language-models-llms/" target="_blank">UpTrain AI</a> -->
        
      </div>

      <div id="matrQA_validity">
        <h2 class="pageTitle">Hallucination</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_hallucination')">Hallucination Degree</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_validity')">Context Validity</button>
          <button class="button button2" onClick="clickCalled('matrQA_disregard')">Context Disregard</button>
        </div>
(This is an LLM Based Evaluator)

Evaluates how relevant the retrieved context is to the question specified.

<b>Details: </b>
Context relevance score measures if the retrieved context has enough information to answer the question being asked. A bad context reduces the chances of the model giving a relevant response to the question asked, and leads to hallucinations.

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question</b>  the query asked by user
<b>context:</b> the information/ text retrieved and input to LLM to answer the question

<b>Calculation:</b>
  • Context relevance is evaluated by determining which of the following three cases apply for given inputs:
    ○ The extracted context can answer the given query completely.
    ○ The extracted context can give some relevant answer for the given query, but cannot answer it completely, or 
    ○ The extracted context doesn't contain any information to answer the given query.
  
<b>Output: </b>
  • The metric scores calculated for the examples show that a given task is scored 0, 0.5 or 1, based on the above.
  
<!-- <b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
Query = "Hello, I'm updating a patient's chart and need to administer care. Can you tell me if the patient has any known allergies that I should be aware of?"

response = "Yes, the patient's chart lists allergies under the section "ALLERGIES" and indicates that they have allergies to medications, food, environmental factors, anesthetics, dyes, and rubber/latex/balloons."

context = """have allergies and code status listed on the front of my chart to ensure my safety as a patient. 2. General Risks. The undersigned understands that the practice of
medicine and surgery is not an exact science and that diagnosis and treatment may involve risks of injury or even death. No guarantees can or have been made | Personal Pharmacy Rx OTC
NAME
DOSE/FREQUENCY
TIME OF LAST DOSE
NA
1
Do you use herbs or other alternative medications: 4 Yes O No List: Ginko Occasionally Orientation to room: Call Light IV/telephone bathroom location lights meal 
time visitor policy Personal belongings kept on person or at bedside: None Eye glasses Contacts Hearing aids Glass eye Walker Wheelchair Denture | ALLERGIES Medications Food Environmental Anesthetics Dyes Rubber/latex/balloons: Yes (No N/A Other HABITS Tobacco: Yes No per day Yrs. Chew: Yes No
per day Yrs. Other Alcohol: Yes No per day yrs. Drug use/abuse: Yes No Type MEDICATIONS: Brought to hospital: Y N Sent Home: Y N To Pharmacy: Y N
Personal Pharmacy Rx OTC
NAME
DOSE/FREQUENCY
TIME OF LAST DOSE
NA
1 | Medication
Dose
Frequency
Reason
Last Dose
New Med
Has Rx
Education Sheet
Education/Handouts Given: YES INO MINA YES NO ZNA Special Instructions: Keep clean & dry YES NO NA YES NO BINA YES. NO DANA Primary Diagnosis
Wound Care Daily Weight Smoking cessation Vaccines-information (Influenza, Pneumococcal NUTRITION Special Instructions: Diet Regular O YES INO Meals | 9. GI . 10. Steroid use 11. Flu Vaccine Current Q N 12. Pneumonia Vaccine current Y (N. if no give patient information. PULSE NKA 82 RESP 20 B/P 134/79 Ht.
5'9". Wt 190.9 lbs 1. Diabetes 2. Epilepsy/seizure disorder 3. High Blood Pressure 4. Heart Disease 5. Kidney Disease 6. Cancer VITALS: TEMP 96.7 | Equipment/Supplies (Provider List) YES NO MINA Transportation Arranged Special Instructions: Patient Signature/Date: John Brown Person Giving
Instructions/Date: Jemy Leurs Physician Signature/Date: 1 White Copy - Chart Yellow Copy - Patient DISCHARGE CHECKLIST: O ADMISSION CONSENT | :unselected: :unselected: :selected: :unselected: :unselected: :selected: :unselected: Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore,
M.D. Consulting Physician Adm Date: 11/12/1999 DOB: 09/10/44 Community General Hospital Anytown, USA DISCHARGE INSTRUCTION SHEET
Medication
Dose
Frequency
Reason
Last Dose
New Med
Has Rx
Education Sheet | Time
Medications Type, Route, Amt., Site
Allergies: NKDA Nurses Notes Warming Blanket O :unselected:
0920-
LR
SUCC in. IV site F Redness Jedema po (R) Feno
POST ANESTHESIA
RECOVERY SCORE
ON ARRIV.
15 MIN.
30 MIN.
45 MIN.
60 MIN.
DIS- CHG
Able to move 4
extremeties voluntarily
or on command = 2
Able to move 2
extremeties voluntarily
or on command = 1
Able to move 0
extremeties voluntarily"""

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Context Validity")
print(eval_score)


<b>Sources:</b>
  • <a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance" target="_blank">Context Relevance - UpTrain</a>
  • <a href="https://blog.uptrain.ai/a-comprehensive-guide-to-context-retrieval-in-llms-2/" target="_blank">A Comprehensive Guide to Context Retrieval in LLMs - UpTrain AI</a>
  • <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context_awareness/relevance.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a>  -->
      </div>

      <div id="matrQA_disregard">
        <h2 class="pageTitle">Hallucination</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_hallucination')">Hallucination Degree</button>
          <button class="button button2" onClick="clickCalled('matrQA_validity')">Context Validity</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_disregard')">Context Disregard</button>
        </div>
<!-- This metric has not been implemented in QT package. -->
(This is an LLM Based Evaluator)

Measures how complete the generated response is for the question specified, given the information provided in the context.

<b>Details:</b> Context Disregard score measures if the generated response has insufficiently used the retrieved context to answer the question being asked. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question:</b>  the query asked by user
<b>context:</b> the information/ text retrieved and input to LLM to answer the question
<b>response:</b> the response given by the model

<b>Calculation:</b>
  • Context disregard is evaluated by determining which of the following three cases apply for given inputs:
    ○ The generated response doesn't incorporate any information present in the context. 
    ○ The generated response incorporates some of the information present in the context, but misses some of the information in context which is relevant for answering the given question.
    ○ The generated response incorporates all the relevant information present in the context.  
  • Context disregard = 1- context utilization.
  • Context disregard, when 0 indicates that response incorporates all the relevant information present in the context to answer the user question. This indicates hallucination is 0. 
  • Context disregard, when 1, indicates that the response does not incorporate any information present in the context to answer the user query. This indicates hallucination is 1. 
  
<b>Output: </b>
A values between 0 and 1.
  
<!-- <b>How to use it?</b>
add code here

<b>Sources:</b>
  • <a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-utilization" target="_blank">Context Utilization - UpTrain</a>
  • <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context_awareness/context_utlization.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a> -->
      </div>
<!-- Robustness -->
      <div id="matrQA_syntax">
        <h2 class="pageTitle">Robustness</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_syntax')">Syntax Sensitivity</button>
          <button class="button button2" onClick="clickCalled('matrQA_lang')">Language Critique</button>
          <button class="button button2" onClick="clickCalled('matrQA_aspect')">Aspect Critique</button>
        </div>
This metric has not been implemented in QT package due to dependency issues.

<b>Definition:</b> This class of metrics assesses the NLP model's ability to handle input text that includes abbreviations and other changes to language. 

<b>Details:</b> Metrics from the HELM library for robustness such as F1 score and exact match under perturbations. Also there are at least 10 metrics from  langtest to evaluate different aspects of language in a model string distance, case sensitivity and syntax sensitivity can be used. 

<b>Library:</b> Langtest HELM

<b>Calculation:</b> These metrics measure how the model performs with changes in the case, abbreviations in the input text. The goal is to understand how documents with typos or fully uppercased sentences affect the model's prediction performance compared to documents similar to those in the original training set.

<b>Inputs:</b> Input text, Expected result

<b>Output:</b> Score between 0 and 1.        
      </div>

      <div id="matrQA_lang">
        <h2 class="pageTitle">Robustness</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_syntax')">Syntax Sensitivity</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_lang')">Language Critique</button>
          <button class="button button2" onClick="clickCalled('matrQA_aspect')">Aspect Critique</button>
        </div>
(This is an LLM Based Evaluator)

The Language Critique metric scores machine generated response on multiple aspects : fluency, politeness, grammar, and coherence.

<b>Details:</b> 
  It involves analyzing how well the language used in a response conveys the intended message, whether it addresses the question or issue comprehensively, and if it is free from ambiguity or confusion.
  • Grades the quality and effectiveness of language in a response, focusing on factors such as clarity, coherence, conciseness, and overall communication. 
  • Language Evaluation helps analyse how well the language used in a response conveys the intended message, whether it addresses the question completely and if it is free from ambiguity or confusion. 
  
<b>Required Arguments: (your dataset must contain these fields)</b>
<b>response:</b> the response given by the application

<b>Calculation:</b>
  • Considers features such as fluent, polite, grammatically correct and coherent, and determine one of the following three cases for evaluation:
    ○ The response is highly rated on these features.
    ○ The response is moderately rated on these features.
    ○ The response is poorly rated on these features. 
    
<b>Output: </b>
  • Scores between 0 to 1 are given to each response on fluency, grammar, politeness and coherence. 
  •  The mean is the overall score for language critique
  
<!-- <b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
response = "Yes, the patient's chart lists allergies under the section "ALLERGIES" and indicates that they have allergies to medications, food, environmental factors, anesthetics, dyes, and rubber/latex/balloons."

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Language Critique")

print(eval_score)

<b>Source:</b>
  • <a href="https://docs.uptrain.ai/predefined-evaluations/language-quality/fluency-and-coherence" target="_blank">Language Features - UpTrain</a>
  • <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/language_features/language_critique.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a> -->
        
      </div>

      <div id="matrQA_aspect">
        <h2 class="pageTitle">Robustness</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_syntax')">Syntax Sensitivity</button>
          <button class="button button2" onClick="clickCalled('matrQA_lang')">Language Critique</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_aspect')">Aspect Critique</button>
        </div>
This metric has not been implemented in QT package due to dependency issues.

(This is an LLM Based Evaluator)

This metric is designed to assess submissions based on predefined aspects such as harmlessness and correctness.

<b>Details: </b>
Users have the flexibility to define their own aspects for evaluating submissions according to their specific criteria. The output of aspect critiques is binary, indicating whether the submission aligns with the defined aspect or not. This evaluation is performed using the 'answer' as input.

<b>Required Arguments:</b> (your dataset must contain these fields)
response: the response given by the application

<b>Calculation:</b>
    ○ Critiques within the LLM evaluators evaluate submissions based on the provided aspect. 
    ○ Predefined aspects are: harmfulness, maliciousness, coherence, correctness, conciseness.
    
<b>Output: </b>
  • A score between 0 to 1
  
<!-- <b>How to use it?</b>
add code here 

<b>Source:</b>
  • <a href="https://docs.ragas.io/en/v0.1.0/concepts/metrics/critique.html" target="_blank">Aspect Critique | Ragas</a> -->
      </div>
<!-- Efficiency -->
      <div id="matrQA_Latency">
        <h2 class="pageTitle">Efficiency</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_Latency')">Latency</button>
          <button class="button button2" onClick="clickCalled('matrQA_cost')">Cost</button>
        </div>
Logs the time taken for a GenAI application to generate a response from the moment it receives an input 
until the output is provided.

Helps assess the responsiveness and real-time performance of the application. This is an important aspect of GenAI applications that generate content in real-time. Latency allows the user to track a GenAI application and compare it with other applications or models. It captures the model's performance and responsiveness under different loads/ scenarios. 

<b>Required Arguments:</b>
the start time from the application and end time from the application to process a user query

<b>Calculation: </b>
    Latency = Time at output – Time at Input

Output is the time in Milliseconds (ms)/ Seconds (s).

<!-- <b>Example code:</b> (change the code)

Latency can be passed as an argument to log response specific metric for an application. To calculate latency, you can refer below example, 

# Function to measure latency for a single request 
def measure_latency(input_text): 
start_time = 
end_time = 
latency = end_time – start_time 

print(latency)

To log latency into Q&T, use the below code

from app.qualitytrust.suite.log_metrics import LogMetrics

log_metrics = LogMetrics()

log_metrics.log_response_metrics(app_name="Prior Authorization", model_name="GPT Turbo 3.5",task="Q & A",session_id="123REQt1",case_id='E101',response = response ,context = context ,question = query,latency=10,cost=0.8)


<b>Note - </b>
  • Latency cannot be calculated in Q&T solution package. We can only log latency of LLM call completion in Q&T solution database.
  • Make sure database and configuration setup is done before.   -->
      </div>

      <div id="matrQA_cost">
        <h2 class="pageTitle">Efficiency</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_Latency')">Latency</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_cost')">Cost</button>
        </div>
Measures the total for a GenAI application to generate a response from the moment it receives an input until the output is provided. This also includes the cost of evaluation of the response. 

<b>Required Arguments:</b>
number of tokens for response, token cost

<b>Calculation: </b>
This metric is calculated in the backend. Consider calculating the number of tokens for a response. Can be calculated using call back function. 

<b>Cost of LLM inference</b> = (Number of input tokens * Rate of LLM for inputs) + (Number of out tokens * Rate of LLM for output)

<b>Output</b> is the cost of the application in dollars 

After calculation, to log calculated LLM cost in Q&T, see the example below:

<!-- <b>How to use it?</b>
from app.qualitytrust.suite.log_metrics import LogMetrics

log_metrics = LogMetrics()

log_metrics.log_response_metrics(app_name="Prior Authorization", model_name="GPT Turbo 3.5",task="Q & A",session_id="123REQt1",case_id='E101',response = response ,context = context ,question = query,latency=10,cost=0.8)         -->
      </div>
<!-- Bias -->
      <div id="matrQA_gender">
        <h2 class="pageTitle">Bias</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_gender')">Gender and Racial Bias</button>
          <button class="button button2" onClick="clickCalled('matrQA_honest')">Honest Score</button>
        </div>
(This is an LLM Based Evaluator)

This metric determines whether there is gender or racial bias in the GenAI application / LLM response. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>input:</b> The user query that is input to the application. 
<b>response:</b> The actual output generated by the GenAI application/ LLM.

<b>Calculation :</b>

The BiasMetric is calculated according to the following equation: 

      Bias = Number of biased opinions
                              Total number of opinions
  
Step 1: Use an LLM to list all the opinions found in the response.
Step 2: Use an LLM to classify each opinion as biased or not.

<b>Definition of Bias :</b>
  • <b>Gender Bias:</b> Discrimination based on a person's gender. 
  • <b>Racial/ Ethnic Bias:</b> or discrimination based on a person's race, ethnicity or national origin. 
  • <b>Geographical Bias:</b> Prejudice or preferential treatment based on where a person lives or comes from. 
<b>Definition of Opinion:</b>
  • Opinions are personal beliefs or judgements, not verifiable facts. 
  • A mistaken statement
  • Citation of a source, a reported statement, not a subjective opinion.

<b>Output: </b>

  A score between 0 and 1. 

<!-- <b>How to use this?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
query = "Could you please confirm Ms. Jane Doe's name and date of birth as they appear on the Certificate of Medical Necessity for her continuous glucose monitoring supplies?"

response = "Ms. Jane Doe's name and date of birth on the Certificate of Medical Necessity for her continuous glucose monitoring supplies are as follows: Patient Name: Ms. Jane Doe Date of Birth: 05/10/1967"

eval_score = eval_obj.evaluate(query=query,response=response,metrics="Gender and Racial Bias")

print(eval_score)


<b>Source: </b>
<a href="https://docs.confident-ai.com/docs/metrics-bias" target="_blank">Bias | DeepEval</a>     -->
      </div>

      <div id="matrQA_honest">
        <h2 class="pageTitle">Bias</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_gender')">Gender and Racial Bias</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_honest')">Honest Score</button>
        </div>
(This is a rule based Evaluator)

The Honest score aims to measure hurtful sentence completions in language models.

<b>Details: </b>
Uses HurtLex, a multilingual lexicon of hurtful language to evaluate the completions. Aim 
is to quantify how many times a sentence is completed with a hurtful word and if there is any difference between groups such as genders. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>response:</b> The actual output generated by the GenAI application/ LLM.

<b>Calculation: </b>
  • Relies on a pre-built dictionary (HurtLex) to measure the hurtful nature of completions
  • The metric matches the words generated by the LLM / response against the HurtLex lexicon to quantify the instances of hurtful completions.
  • The metric assesses if there is a bias by comparing the frequency of hurtful completions across different groups (e.g., genders, sexual orientations).

<b>Output: </b>
A score between 0 and 1. 

<!-- <b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Honest Score")

print(eval_score)

<b>Source:</b>
  • <a href="https://huggingface.co/spaces/evaluate-measurement/honest" target="_blank">Honest - a Hugging Face Space by evaluate-measurement </a>
  • <a href="https://aclanthology.org/2021.naacl-main.191.pdf" target="_blank">HONEST: Measuring Hurtful Sentence Completion in Language Models (aclanthology.org)</a> -->
        
      </div>
<!-- Toxicity -->
      <div id="matrQA_toxic">
        <h2 class="pageTitle">Toxicity</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_toxic')">Toxic Opinions</button>
          <button class="button button2" onClick="clickCalled('matrQA_abuse')">Abusive Speech</button>
        </div>
(This is an LLM Based Evaluator)

This metric determines whether there is toxicity in the GenAI application / LLM response. Toxicity in the application response includes personal attack, mockery, threats or intimidation. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>input:</b> The user query that is input to the application. 
<b>response:</b> The actual output generated by the GenAI application/ LLM.

<b>Calculation :</b>

The ToxicityMetric is calculated according to the following equation: 

    Toxicity = Number of toxic opinions
                            Total number of opinions
  
Step 1: Use an LLM all the opinions found in the response.
Step 2: Use an LLM to classify each opinion as toxic or not.

<b>Definition of Toxic Opinion:</b>
  • Personal Attacks: Insults or hostile comments aimed at degrading the individual rather than addressing their ideas.
  • Mockery: Sarcasm or ridicule used to belittle someone.
  • Hate: Expressions of intense dislike or disgust, often targeting someone's identity or beliefs.
  • Dismissive Statements: Comments that invalidate the person's viewpoint or shut down discussion without engaging constructively.
  • Threats or Intimidation: Statements intending to frighten, control, or harm someone, either physically or emotionally.
  
<b>Output: </b>
  The percentage of toxic opinions in the response. A number between 0 and 1. 

<!-- <b>Example: </b>

#Input data to calculate this metric 

from qualitytrust.qnteval import EvaluateMetric

eval_obj = EvaluateMetric()
  
query="What are the limitations that substantially affect Ms. Jane Doe's daily life due to her diabetes complications?"

response = "Ms. Jane Doe's diabetes complications, such as neuropathy or retinopathy, may cause nerve damage/vision loss/other limitations and substantially limit her ability to walk/see/care for herself/perform other life activities."

eval_score = eval_obj.evaluate(query=query,response=response,metrics="Toxic Opinions")


print(eval_score)

<b>Source: </b>
<a href="https://docs.confident-ai.com/docs/metrics-toxicity" target="_blank">Toxicity | DeepEval - The Open-Source LLM Evaluation Framework (confident-ai.com)</a> -->
        
      </div>

      <div id="matrQA_abuse">
        <h2 class="pageTitle">Toxicity</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_toxic')">Toxic Opinions</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_abuse')">Abusive Speech</button>
        </div>
(This is an LLM based evaluator)

The toxicity measurement aims to quantify the toxicity of the input texts using a pretrained hate speech classification model. The default model used is roberta-hate-speech-dynabench-r4. In this model, ‘hate’ is defined as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” 

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>Input Text:</b> The actual output generated by the GenAI application/ LLM. Is a collection of sentences.

<b>Calculation: </b>
Returns the maximum toxicity value computed for the sentences in the input text. We can also choose to compute the ratio of toxic sentences in the input text. 

<b>Output:  </b>
Returns: 
float or list: The toxicity score(s) computed. If `dataset_flag` is False, a single float value is returned. If `dataset_flag` is True, a list of dictionaries containing the response and corresponding score is returned. 

<!-- <b>Example:</b> Also does this metric need context and query? The metric definition does not specify!

from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Abusive Speech")

print(eval_score)

<b>Source:</b>
<a href="https://github.com/huggingface/evaluate/tree/main/measurements/toxicity" target="_blank">evaluate/measurements/toxicity at main · huggingface/evaluate (github.com)</a> -->
        
        
      </div>

      <div id="matrClinic_defect">
        <h2 class="pageTitle">Healthcare Specific Metrics</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrClinic_defect')">Clinical Defect Severity</button>
          <button class="button button2" onClick="clickCalled('matrClinic_density')">Clinical Concept Density Score</button>
          <button class="button button2" onClick="clickCalled('matrClinic_relation')">Clinical Relationship Overlap Score</button>
          <button class="button button2" onClick="clickCalled('matrClinic_edit')">Clinical Edit Rate </button>
        </div>
(LLM Based Evaluator) 

Clinical Hallucination Severity measures the severity of defects in the summary. It aims to rate the harm caused by the issues in the LLM generated summary. 

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>document:</b> The source document that contains the information that should be summarized. 

<b>reference summary:</b> SME generated reference summary. 

<b>response/ summary:</b> The LLM generated summary of the source document. 

<b>Calculation: </b>

Fine-tune the prompt on reference summary or source document.  
Compare source document with the LLM generated summary. 
Assign score based on severity of defect scale below: 

    Low impact: The defect is so immaterial that it is unlikely to be noticed.  

    Mild impact: The defect could potentially lead to misunderstandings or miscommunication but is still just annoying rather than actively harmful.  

    Moderate impact: The defect poses a risk of moderate harm if not corrected, for example omitting some detail about a symptom relevant to the chief complaint.  

    Major impact: The defect poses a significant risk of causing incorrect treatment or diagnosis, demanding immediate correction.  

    Critical impact: The defect could lead to serious adverse patient outcomes without correction, such as incorrect surgery or medication. 

<b>Output: </b>

A severity score on the defect severity scale.         
      </div>

      <div id="matrClinic_density">
        <h2 class="pageTitle">Healthcare Specific Metrics</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrClinic_defect')">Clinical Defect Severity</button>
          <button class="button button2 active" onClick="clickCalled('matrClinic_density')">Clinical Concept Density Score</button>
          <button class="button button2" onClick="clickCalled('matrClinic_relation')">Clinical Relationship Overlap Score</button>
          <button class="button button2" onClick="clickCalled('matrClinic_edit')">Clinical Edit Rate </button>
        </div>      
(NER based evaluator) 

Maps the concepts between summary and source documents. Evaluates the entity mapping.   

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>document:</b> The source document that contains the information that should be summarized. 

<b>response/ summary:</b> The LLM generated summary of the source document. 

<b>Calculation: </b>

Entity/ concept extraction from both the summary and the source document.  
Map the entities in the summary with the entities in the source document.  

<b>Output: </b>

A score capturing the proportion of entities in the summary mapped to the source document.  
                
      </div>

      <div id="matrClinic_relation">
        <h2 class="pageTitle">Healthcare Specific Metrics</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrClinic_defect')">Clinical Defect Severity</button>
          <button class="button button2" onClick="clickCalled('matrClinic_density')">Clinical Concept Density Score</button>
          <button class="button button2 active" onClick="clickCalled('matrClinic_relation')">Clinical Relationship Overlap Score</button>
          <button class="button button2" onClick="clickCalled('matrClinic_edit')">Clinical Edit Rate </button>
        </div>      
(LLM based evaluator) 

This metric compares the percentage of overlap in relationships extracted from the summary and the source document.  

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>document:</b> The source document that contains the information that should be summarized. 

<b>response/ summary:</b> The LLM generated summary of the source document. 

<b>Calculation: </b>

Entity/ concept and relationship extraction from both the summary and the source document.  
Map the relationships in the summary with those in the source document.  

<b>Output: </b>

A score capturing the proportion of relationships in the summary mapped to the source document.       
      </div>

      <div id="matrClinic_edit">
        <h2 class="pageTitle">Healthcare Specific Metrics</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrClinic_defect')">Clinical Defect Severity</button>
          <button class="button button2" onClick="clickCalled('matrClinic_density')">Clinical Concept Density Score</button>
          <button class="button button2" onClick="clickCalled('matrClinic_relation')">Clinical Relationship Overlap Score</button>
          <button class="button button2 active" onClick="clickCalled('matrClinic_edit')">Clinical Edit Rate </button>
        </div>      
(LLM / NER based evaluator) 

This metric captures the percentage of edits (or incorrect details detected by an SME/ user feedback) to the summary made by the clinician/SME categorized by clinical concepts/ entities. 

entities/ words: number of medical entities recognized by an NER model or number of words in the input text 
edits: the number of edits made by the user to the input text. 

<b>Calculation: </b>

This metric takes as input the number of concepts and entities in the input text and the number of edits made to those concepts and entities and calculates the proportion.  
Use an NER model to identify the edits for potential harm associated with hallucinations. 

<b>Output: </b>

A score between 0 and 1 for each of the concept categories. 
Example output: 
Disease : 0.02 
Medication :  0.05. 
      </div>

      <div id="matrSum_rouge">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>        
(This is a Reference Based Evaluator)  

ROUGE score measures the overlap of words or phrases (n-grams) between the generated answer and reference answer. 

<b>Details:  </b>

Recall-Oriented Understudy for Gisting Evaluation. It is used to assess the quality of automatic summarization systems. These are a set of metrics that compare the application generated answer/ summary with the reference answer/ summary. ROUGE is case insensitive.  
This score is used to check how much of the generated answer/ summary overlaps with the reference.  
  
<b>Types of Rouge score:  </b>

<b>ROUGE-1 :</b> unigram (1-gram) based scoring, I.e. measures that overlap of individual words.  

<b>ROUGE-2 :</b> specifically evaluates the overlap of bigrams between the system-generated output and reference summaries, I.e. measures the overlap of pairs of words.  

<b>ROUGE-L :</b> Longest common subsequence based scoring, I.e. measures the longest sequence of matching words. Ignores newlines and computes LSC for the entire text.  

<b>ROUGELsum:</b> splits text using "\n". This is a variant of the ROUGE-L metric. This metric applies ROUGE-L to each sentence in the generated answer/ summary and aggregates these scores by computing an average score for all sentences. Suitable for extractive summarization tasks.  

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>application response :</b> the answer / summary given by the application in response to the user query.  

<b>reference:</b> the ground truth / SME generated summary for the source document 

<b>Calculation: </b>

ROUGE-L calculates the longest common subsequence by ignoring newlines.  
Compares the text in the generated answer/ summary with the reference.  

<b>Output:  </b>

Output is a score between 0 to 1, where 0 indicates no overlap between bigrams and 1 indicates a perfect match. 

<b>Limitations: </b>

ROUGE score doesn't capture semantic meaning. 
May not handle paraphrasing or synonym usage well.                  
      </div>

      <div id="matrSum_rougeWE">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>        
(This is a Reference Based Evaluator) 

ROUGE-WE (ROUGE- Word Embeddings) is an extension of ROUGE that incorporates word embeddings to capture semantic similarity between words.  

<b>Details:   </b>

ROUGE-WE uses pre-trained word embeddings (e.g., Word2Vec, GloVe) to represent words as vectors in a high-dimensional space. 
Instead of just matching exact words, ROUGE-WE measures the semantic similarity between words (using word embeddings) in the generated summary and the reference summary. 
ROUGE-WE can be applied to different ROUGE variants like ROUGE-N (n-gram overlap), ROUGE-L (longest common subsequence), etc., but with a focus on semantic similarity. 

<b>Required Arguments: (your dataset must contain these fields) </b>

response/ summary: The LLM generated summary of the source document. 
reference summary: the ground truth / SME generated summary for the source document. 
embeddings model: model to be used. 

<b>Calculation: </b>

<b>Step 1:</b> Convert words in both the generated summary and the reference summary to their corresponding word embeddings. 

<b>Step 2: </b> the cosine similarity between the embeddings of words in the generated summary and the reference summary. 

<b>Step 3:</b> Aggregate the similarity scores to compute the final ROUGE-WE score. This can be done using various methods like averaging the maximum similarity scores for each word. 

<b>Output:  </b>

A number between 0 and 1.  
      </div>

      <div id="matrSum_meteor">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>        
(Reference based Evaluator) 

METEOR is based on the concept of unigram matching between the LLM produced summary and the reference summary.  

<b>Details: </b>

Maps unigrams based on surface forms. 
Performs stemming if surface forms do not match.  

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>response/ summary/ candidate:</b> The LLM generated summary of the source document. 
<b>reference summary:</b> the ground truth / SME generated summary for the source document. 

<b>Calculation:  </b>

<b>Step 1:</b> Unigram precision (P) is computed as the ratio of number of unigrams in the LLM summary that are mapped (to the reference) to the total number of unigrams in the LLM summary.  

<b>Step 2:</b> Unigram recall (R) is computed as the ratio of number of unigrams in the LLM summary that are mapped (to unigrams in reference) to the total number of unigrams in reference.  

<b>Step 3:</b> Calculate Fmean defined as the harmonic mean of precision and recall.  

<b>Step 4:</b> METEOR penalizes unigram matches, i.e. larger the matching subsequences between the two texts, lower the penalty.  

METEOR score = Fmean * (1-Penalty) 

(This has the effect of reducing the Fmean by maximum of 50% if there are no bigram or longer matches) 

<b>Output: </b>

A number between 0 and 1.          
      </div>

      <div id="matrSum_mover">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>        
(This is a Reference Based Evaluator)  

MoverScore is a reference-based evaluation metric. It evaluates text generation with contextualized embeddings and word mover distance. 

<b>Details: </b>

Compares system summary against reference based on their semantic rather than surface form.  
Metric assigns perfect score to the summary if it conveys the same meaning as the reference text.  
Any deviation from reference summary leads to a reduced score.  

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>response/ summary:</b> The LLM generated summary of the source document. 
<b>reference summary:</b> the ground truth / SME generated summary for the source document. 

<b>Calculation:  </b>

<b>Step 1:</b> Compute contextualized representation of the reference and LLM generated summary. 

<b>Step 2:</b> Compute the  distance between these representations measuring the semantic distance between LLM generated summary and reference. Word Mover's Distance is used.  

  This distance is used to reflect to what extent the LLM generated text has deviated from reference.  

<b>Step 3:</b> Find the minimum effort to transform between two sentences / texts by calculating the Mover score. 

<b>Output: </b>

A single score between 0 and 1, indicating the similarity of any system generated text with the reference. 
      </div>

      <div id="matrSum_sentence">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>        
(This is a Reference based Evaluator)

Sentence Mover's Similarity metric evaluates text using word and sentence embeddings.

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>response/ summary/ candidate:</b> The LLM generated summary of the source document.

<b>reference summary:</b> The ground truth / SME generated summary for the source document.

<b>Calculation:</b>

Steps to calculate Sentence Mover's Similarity Score: 

<b>Step 1:</b> Represent Sentences with Embeddings by converting each sentence into numerical vectors using word embeddings. 

<b>Step 2:</b> Weight Sentence Embeddings by weighting each sentence based on the number of words. 

<b>Step 3:</b> Formulate a Linear Optimization Problem to minimize the cost of transforming embeddings from Document A to Document B. 

<b>Step 4:</b> Define the Vocabulary to consist of the sentence embeddings from both documents. 

<b>Step 5:</b> Solve the linear optimization problem to find the best match between the sentences. 

<b>Step 6:</b> Then calculate the similarity score from the minimized distance. 

<b>Output:</b>

A number between 0 and 1.
      </div>

      <div id="matrSum_bert">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>        
(This is a BERT based, Reference based Evaluator) 

BERTScore computes a similarity score for each token in the LLM generated summary with each token in the reference summary.  

<b>Details: </b>

It evaluates semantic equivalence.  
Based on pre-trained BERT contextual embeddings.  
Contextual embeddings capture the specific use of a token in a sentence, and potentially capture sequence information.  
BERTscore computes the similarity of two sentences using cosine similarities between their token embeddings.  

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>response/ summary/ candidate:</b> The LLM generated summary of the source document. 
<b>reference summary:</b> The ground truth / SME generated summary for the source document. 

<b>Calculation:  </b>

<b>Step 1:</b> Use BERT model to tokenize the reference and candidate sentences.  

<b>Step 2:</b> Use contextual embeddings to represent the tokens. The embedding model generates a sequence of vectors.  

<b>Step 3:</b> The vector representation is used to compute similarity. Compute cosine similarity of a reference token with a candidate token. 

<b>Step 4:</b> BERTscore matches each token in reference with each token in candidate sentence to compute recall. Each token in candidate with each token in reference to compute precision. Both are combined to compute F1 score also BERTscore. 

<b>Output: </b>

A number between 0 and 1.  
      </div>

      <div id="matrSum_agreement">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div> 
(This is an LLM Based Evaluator)

Agreement score metric compares information in a summary and the source document and gives a score for agreement between the summary and the source document.  

This evaluator compares information between a source document and an LLM generated summary. The evaluator then generates close ended questions from the summary. It compares the answers from the summary and the document. Agreement Score is the percentage of questions that had identical answers for both summary and document.

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>document:</b> The source document that contains the information that should be summarized.
<b>response/ summary:</b> The LLM generated summary of the source document.
  
<b>Calculation (include input, calculation/ formula, output details)</b>

  The evaluator compares the information between a source document and an LLM generated summary.
  
  <b>Step 1:</b> Generates N closed-ended (Y/N/Unknown) questions from the summary information.
  <b>Step 2:</b> QuestionAnswerer LLM is used to answer each question given ONLY the summary as context.
  <b>Step 3:</b> QuestionAnswerer LLM is used to answer each question given ONLY the source document as context.
  <b>Step 4:</b> Compare the answers from the summary and document for each question to find contradictions.

<b>Output: </b>
  The percentage of questions that had identical answers for both contexts. A number between 0 and 1. 
  
<b>Algorithm Elaborated</b>

<img src="images/matrSum_agreement.png">

<B>Configuration Options:   </B>

n_questions: int 

Number of questions to generate. 

More questions = more accurate, granular evaluations, but it will also mean higher evaluation time, and LLM inference cost. 

questions: List[str] 

If you would like to ask custom questions instead of generating the questions, you can provide a list of questions using the constructor argument questions. 

question_answerer: QuestionAnswerer (this is a healthcare specific question answer generator) 

You can also configure which LLM prompting technique to use for answering questions: 

QuestionAnswererBulk (faster, cheaper, default): uses a single prompt to answer all the questions. 

QuestionAnswererChainOfThought (slower, uses more tokens, better reasoning): will prompt the LLM separately for each question, wrapped in a chain of thought prompt. 

QuestionAnswererWithRetrieval: (good for large documents) uses a simple similarity search to narrow-down context.         
      </div>

      <div id="matrSum_coverage">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div> 
(This is an LLM Based Evaluator)

The summarization metric uses LLMs to determine whether your LLM (application) is generating factually correct summaries while including the necessary details from the original text. The coverage score is a part of the summarization metric and determines whether the summary contains the necessary information from the original text. It quantifies how well a summary captures and accurately represents key information from the original text, with a higher score indicating greater comprehensiveness.

Calculation (include input, calculation/ formula, output details)
  Focuses on coverage of details from the original text. Measures the amount of detail included in the summary from the original text. 

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>original text/ input:</b> The source document that contains the information that should be summarized.
<b>actual output:</b> The LLM generated summary of the source document.

Step 1. n close ended questions are generated from the original text that can only be answered with a 'yes' or 'no'.

Step 2: Calculate the ratio of questions to which both the original text and the summary give the same answer.

Step 3: Higher the number of matching answers, greater the coverage score. Matching answers indicate the summary is both factually correct and contains sufficient detail to answer the question. 

<b>Output: </b>
The percentage of questions that had identical answers for both contexts. A number between 0 and 1. 


<b>Configuration Options:</b>
There are eight optional parameters when instantiating an SummarizationMetric class:

Note: Set the strict mode to True to indicate a perfect result and to reject any other score. 

• [Optional] threshold: the passing threshold, defaulted to 0.5.  
• [Optional] assessment_questions: a list of close-ended questions that can be answered with either a 'yes' or a 'no'. These are questions you want your summary to be able to ideally answer, and is especially helpful if you already know what a good summary for your use case looks like. If assessment_questions is not provided, a set of assessment_questions are generated at evaluation time. The assessment_questions are used to calculate the coverage_score.
• [Optional] n: the number of assessment questions to generate when assessment_questions is not provided. Defaulted to 5.
• [Optional] model: a string specifying which of OpenAI's GPT models to use, OR any custom LLM model of type DeepEvalBaseLLM. Defaulted to 'gpt-4o'.
• [Optional] include_reason: a boolean which when set to True, will include a reason for its evaluation score. Defaulted to True.
• [Optional] strict_mode: a boolean which when set to True, enforces a strict evaluation criterion. In strict mode, the metric score becomes binary: a score of 1 indicates a perfect result, and any outcome less than perfect is scored as 0. Defaulted as False.
• [Optional] async_mode: a boolean which when set to True, enables concurrent execution within the measure() method. Defaulted to True.
(async_mode = True indicates that the step 2, where the answers are extracted from both the document and the summary/ any steps that are independent will be executed concurrently, while False will cause them to be executed sequentially).
• [Optional] verbose_mode: a boolean which when set to True, prints the intermediate steps used to calculate said metric to the console, as outlined in the How Is It Calculated section. Defaulted to False. 
      </div>

      <div id="matrSum_summacc">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>
(This is a NLI model based evaluator)

Evaluates factual consistency of summary with input documents. It is based on the aggregation of sentence-level entailment scores for each pair of input document and summary sentences.

<b>Details: </b>

  • This metric measures the factual consistency of the summary with the input document. 
  • Uses natural language inference models for inconsistency detection.
  • Provides a method called SummaCConv that enables NLI models to be used for summarization by segmenting documents into sentences and aggregating scores between pairs of sentences. 
  • SummaCConv is a trained model consisting of a single learned convolution layer compiling the distribution of entailment scores of all document sentences into a single score. 

<b>Calculation:</b>
  • Generate an NLI pair matrix by splitting the document and the summary into sentence blocks. 
  • The document is split into M blocks, and the summary into N blocks. 
  • Each combination (Mi,Nj) is run through the NLI model to generate a probability distribution for (entailment, contradiction, neutral).
  • For each summary sentence, the score for the document sentence that provides the strongest support for each summary sentence is retained. 
  •  SummaCConv : Each summary sentence is scored and an average is obtained as the final summary-level score. 

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>original text/ input:</b> The source document that contains the information that should be summarized.
<b>actual output:</b> The LLM generated summary of the source document.

<b>Output: </b>

  A number between 0 and 1.         
      </div>

      <div id="matrSum_summaqa">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>
(Question Answering based, reference free evaluator) 

SummaQA is a set of measures for evaluating a summary based on question answering. 

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>document:</b> The source document that contains the information that should be summarized. 

<b>response/ summary:</b> The LLM generated summary of the source document. 

<b>Methodology: </b>

Use BERT- base pretrained model, finetuned on the SQuAD dataset.  

Unsupervised metrics QA(fscore) and QA(conf) are proposed. 

<b>Calculation: </b>

Propose metrics to better predict the quality of summaries. Can be used for abstractive summarization. Human generated reference summaries are not required. 

Step 1: Questions are generated from reference summary (by masking each of the named entities present).  

Step 2: Triples (input, question, answer) are generated, where input is the source document, question is the sentence containing masked entity and answer is the masked entity to be retrieved.  

Step 3: For each triple, an F1 score, is computed according to responses retrieved by the QA system. This score measures the overlap between the predictions and the ground truth answers. An average F1 score denoted by QA (fscore) is then computed over all triples.  

Step 4: Another metric, QA confidence is provided to consider the confidence of the QA system for its retrieved answer. This corresponds to the probability of the true answer according to the QA model. Confidence scores are averaged for each summary over all the triples denoted by QA(conf). 

<b>Output:  </b>

QA (fscore) is average F1 score between 0 and 1,  
QA (conf) is the average confidence over all responses to questions by question generator, is a number between 0 and 1.  
      </div>

      <div id="matrSum_consistency">
        <h2 class="pageTitle">Accuracy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_rouge')">Rouge Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_rougeWE')">Rouge-WE</button>
          <button class="button button2" onClick="clickCalled('matrSum_meteor')">METEOR</button>
          <button class="button button2" onClick="clickCalled('matrSum_mover')">Mover Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_sentence')">Sentence Mover's Similarity</button>
          <button class="button button2" onClick="clickCalled('matrSum_bert')">BERT Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_agreement')">Agreement Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_coverage')">Coverage Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_summacc')">SummaCConv</button>
          <button class="button button2" onClick="clickCalled('matrSum_summaqa')">SummaQA</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_consistency')">Consistency</button>
        </div>
(This is an LLM Based Evaluator) 

Evaluates if the summary includes only important information and excludes redundancies. 

<b>Details: </b>

Scores are generated from gpt-4 (any GPT model) using pre-defined prompts. 

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>document:</b> The source document that contains the information that should be summarized. 

<b>response/ summary:</b> The LLM generated summary of the source document. 

<b>Calculation : </b>

Prompts are crafted for this criteria, taking the source document and the summary as inputs, and leveraging chain-of-thought generation and guiding the model to output a numeric score from 1-5 for the criteria. 

Step 1: Uses prompts to instruct an LLM to score the summary on relevancy.  
Step 2: Uses a direct scoring function where gpt-4/ any LLM generates a discrete score (1-5) for the metric. 
Step 3: Converts it to a metric between 0 and 1.  

<b>Output:  </b>

Is a value between 0 and 1. The higher the value, more relevant the summary to the document.  
      </div>

      <div id="matrSum_supert">
        <h2 class="pageTitle">Relevancy</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrSum_supert')">SUPERT</button>
          <button class="button button2" onClick="clickCalled('matrSum_relevance')">Summary Relevance</button>
        </div>
SUPERT rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. 

<b>Details: </b>
  • Objective is to evaluate multi-document summaries without using human annotations or reference summaries. 
  • Focus on measuring how much of information from the source documents is covered by the summary.

<b>Calculation:</b>
  • SUPERT evaluates the quality of a summary by measuring its semantic similarity to a pseudo reference summary. 
  • This pseudo reference is created by selecting salient sentences from the source documents.

<b>Steps involved in calculation:</b>
  • Identify the salient information in the input documents, to build a pseudo reference. 
  • Measure the semantic overlap between the pseudo reference and the summary to be evaluated. 
  • The resulting evaluation is called SUPERT (SUmmarization evaluation with Pseudo references and bERT).
  • Proposed method, Top+Clique to build pseudo summaries:
    ○ Label top-N sentences from each document as salient. 
    ○ With the remaining (non-top-N) sentences, build a graph such that only "highly similar" sentences have an edge between them. 
    ○ Obtain the cliques from the graph and select the semantically central sentence from each clique as potentially salient sentences. 
    ○ For each potentially salient sentence, label as salient if it is not highly similar to any top-N sentences. 
    ○ N = 10, and threshold for highly similar is 0.75. 
  • Note: A clique in a graph is a group of nodes (or in this case, sentences) where every node is directly connected to every other node in the group. In simpler terms, a clique here is a set of sentences that are all closely related or similar to each other.
  • The most central sentence is the one that's the most similar to the other sentences in that clique.
  • Use SBERT to measure the similarity between the summary and the pseudo reference. 
  • SUPERT can be used as rewards to train an RL - based - summarizer. 
<!-- <b>Required Arguments: (your dataset must contain these fields)</b>

<b>original text/ input:</b> The source document that contains the information that should be summarized.
<b>actual output:</b> The LLM generated summary of the source document. -->
<b>Output: </b>

  A number between 0 and 1. 
      </div>

      <div id="matrSum_relevance">
        <h2 class="pageTitle">Relevancy</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_supert')">SUPERT</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_relevance')">Summary Relevance</button>
        </div>
(This is an LLM Based Evaluator)

Evaluates if the summary includes only important information and excludes redundancies.

<b>Calculation (include input, calculation/ formula, output details)</b>
  Prompts are crafted for this criteria, taking the original document and the summary as inputs, and leveraging chain-of-thought generation and guiding the model to output a numeric score from 1-5 for the criteria.
  
  Scores are generated from gpt-4 with the defined prompts, comparing them across summaries.

<b>Required Arguments: (your dataset must contain these fields)</b>
  <b>original text/ input:</b> The source document that contains the information that should be summarized.
  <b>actual output:</b> The LLM generated summary of the source document.
  
  Step 1: Uses prompts to instruct an LLM to score the summary on relevancy. 
  Step 2: Uses a direct scoring function where gpt-4/ any LLM generates a discrete score (1-5) for the metric.
  Step 3: Converts it to a metric between 0 and 1. 
  
  Output: Is a value between 0 and 1. The higher the value, more relevant the summary to the document. 
  
      </div>

      <div id="matrSum_hallucination">
        <h2 class="pageTitle">Hallucination</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrSum_hallucination')">Hallucination Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_misalignment')">Misalignment Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_factCC')">FactCC</button>
        </div>
(This is an LLM Based Evaluator)

This evaluator checks  for inaccuracies or hallucinated information in the summaries. The evaluator compares information between a source document and an LLM generated summary. The evaluator then generates close ended questions from the summary. It compares the answers from the summary and the document. Hallucination Score is the percentage of questions where the summary answered definitively but the source document answered "unknown". 

This evaluator is to check mismatches / instances where a summary generated by a model provides a clear answer (either "Yes" or "No") to a question based on its content, while the source document indicates uncertainty or lack of clarity (answering "Unknown").

For example, if the source says, "It is unknown if the patient has a specific disease," and the summary says, "The patient does not have the disease," this is problematic because the source did not provide enough evidence to support the definitive "No" claim. In this case, the model introduced new information or assumed certainty that did not exist in the original data, which would be considered hallucination.

<b>Required Arguments: (your dataset must contain these fields)</b>

  <b>document:</b> The source document that contains the information that should be summarized.
  <b>response:</b> The LLM generated summary of the source document.

<b>Calculation:</b>
  The evaluator compares the information between a source document and an LLM generated summary.
  
  Step 1: Generates N closed-ended (Y/N/Unknown) questions from the summary information.
  Step 2: QuestionAnswerer LLM is used to answer each question given ONLY the summary as context.
  Step 3: QuestionAnswerer LLM is used to answer each question given ONLY the source document as context.
  Step 4: Compare the answers from the summary and document for each question to find the questions for which the summary gave a definitive – Yes/No answer, but the source document answered unknown. 

<b>Output: </b>
  The percentage of questions where the summary answered definitively but the source document answered "unknown". A number between 0 and 1. 

<b>Configuration Options: (these configuration options have not been included in Q&T solution version 1)</b>
  n_questions: int
  Number of questions to generate.
  More questions = more accurate, granular evaluations, but it will also mean higher evaluation time, and LLM inference cost.
  
  questions: List[str]
  If you would like to ask custom questions instead of generating the questions, you can provide a list of questions using the constructor argument questions.
  
  question_answerer: QuestionAnswerer
  You can also configure which LLM prompting technique to use for answering questions:
  QuestionAnswererBulk (faster, cheaper, default): uses a single prompt to answer all the questions.
  QuestionAnswererChainOfThought (slower, uses more tokens, better reasoning): will prompt the LLM separately for each question, wrapped in a chain of thought prompt.
  QuestionAnswererWithRetrieval: (good for large documents) uses a simple similarity search to narrow-down context.
        
      </div>

      <div id="matrSum_misalignment">
        <h2 class="pageTitle">Hallucination</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_hallucination')">Hallucination Score</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_misalignment')">Misalignment Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_factCC')">FactCC</button>
        </div>
(This is an LLM Based Evaluator)

The summarization metric uses LLMs to determine whether your LLM (application) is generating factually correct summaries while including the necessary details from the original text. The alignment score is a part of the summarization metric and determines whether the summary is factually aligned with the original text. 

<b>Calculation:</b>
  Focuses on factual alignment between the original text and summary. 

<b>Required Arguments: (your dataset must contain these fields)</b>
  <b>original text/ input:</b> The source document that contains the information that should be summarized.
  <b>actual output:</b> The LLM generated summary of the source document.
  
  Step 1. n close ended questions are generated from the summary that can only be answered with a 'yes' or 'no'.
  
  Step 2: Calculate the ratio of questions to which both the original text and the summary give the same answer.
  
  Step 3: Higher the number of matching answers, greater the alignment score. Matching answers indicate the summary is both factually correct and contains sufficient detail from the original text. 
  
  Note: In case of the alignment metric, we generate close-ended questions from the summary and not the document. This is because we want to detect cases of hallucination and contradiction, so we are more interested in the original text's answers to the questions generated from the summary. An "I don't know" or "no" answer from the original text's indicates a hallucination or contradiction. The answers from the summary are all expected to be yes.
  
  Step 4: (Optional):
  Manually supply a set of assessment questions used for step 3 above, if you know the type of summary that is expected. 

<b>Output: </b>
  The percentage of questions that had identical answers for both contexts. A number between 0 and 1. 

<b>Configuration Options:</b>
There are eight optional parameters when instantiating an SummarizationMetric class:
Note: Set the strict mode to True to indicate a perfect result and to reject any other score. 
  • [Optional] threshold: the passing threshold, defaulted to 0.5.
  • [Optional] assessment_questions: a list of close-ended questions that can be answered with either a 'yes' or a 'no'. These are questions you want your summary to be able to ideally answer, and is especially helpful if you already know what a good summary for your use case looks like. If assessment_questions is not provided, a set of assessment_questions are generated at evaluation time. The assessment_questions are used to calculate the coverage_score.
  • [Optional] n: the number of assessment questions to generate when assessment_questions is not provided. Defaulted to 5.
  • [Optional] model: a string specifying which of OpenAI's GPT models to use, OR any custom LLM model of type DeepEvalBaseLLM. Defaulted to 'gpt-4o'.
  • [Optional] include_reason: a boolean which when set to True, will include a reason for its evaluation score. Defaulted to True.
  • [Optional] strict_mode: a boolean which when set to True, enforces a strict evaluation criterion. In strict mode, the metric score becomes binary: a score of 1 indicates a perfect result, and any outcome less than perfect is scored as 0. Defaulted as False.
  • [Optional] async_mode: a boolean which when set to True, enables concurrent execution within the measure() method. Defaulted to True.
  (async_mode = True indicates that the step 2, where the answers are extracted from both the document and the summary/ any steps that are independent will be executed concurrently, while False will cause them to be executed sequentially).
  • [Optional] verbose_mode: a boolean which when set to True, prints the intermediate steps used to calculate said metric to the console, as outlined in the How Is It Calculated section. Defaulted to False. 
      </div>

      <div id="matrSum_factCC">
        <h2 class="pageTitle">Hallucination</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_hallucination')">Hallucination Score</button>
          <button class="button button2" onClick="clickCalled('matrSum_misalignment')">Misalignment Score</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_factCC')">FactCC</button>
        </div>
(BERT Based, Reference free Evaluator) 

Factual Consistency Checking model (FactCC) is a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and an LLM generated summary.  

<b>Details: </b>

The metric is designed to assess the factual accuracy of abstractive summaries by checking if the information in the summary is consistent with the source document. It ensures that a summary contains statements that are entailed by the source document.  

<b>Required Arguments: (your dataset must contain these fields) </b>

<b>original text/ input :</b> The source document that contains the information that should be summarized. 

<b>summary:</b> The LLM generated summary of the source document. 

<b>Metric Details: </b>

Focuses on factual adherence to information provided in the source document. A document-sentence approach for factual consistency checking, where each sentence of the summary is verified against the entire body of the source document.  

<b>Model training process:  </b>

Step 1: Input is annotated summaries and input documents. Training data is extracted from documents by applying rule based transformations, generating both consistent and inconsistent examples.  
Step 2: Sample single sentences from the document (also referred to as claims) and  
Step 3: Claims pass through a set of textual transformations to output new sentences with positive and negative labels. (Examples of text transformations include: Paraphrasing, Sentence Negation, Pronoun Swap, Entity Swap, Number swap and Noise injection for artificially generating training data). 
Step 4: The model (BERT-based) is trained on above data to classify sentences as factually consistent or inconsistent. This is called FactCC. 
Step 5: Additionally span selection heads were trained to extract span in the summary sentence that is inconsistent if one exists (out of scope in FactCC). 

<b>Calculation: </b>

Document-sentence approach, where each sentence of the summary is verified against entire body of the source document.  

Step 1: Give the source document and the summary as input to the model.  
Step 2: Model verifies each sentence in the summary against the entire body of the source document. 

<b>Output:  </b>

Model gives a score to the summary and gives a label: Correct and Incorrect. A number between 0 and 1.  
      </div>

      <div id="matrSum_coherance">
        <h2 class="pageTitle">Robustness</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrSum_coherance')">Coherence</button>
          <button class="button button2" onClick="clickCalled('matrSum_fluency')">Fluency</button>
        </div>
(This is an LLM Based Evaluator)

Assesses the logical flow and organization of the summary.

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>original text/ input:</b> The source document that contains the information that should be summarized.
<b>actual output:</b> The LLM generated summary of the source document.

Step 1: Uses prompts to instruct an LLM to score the summary on coherence. 
Step 2: Uses a direct scoring function where gpt-4/ any LLM generates a discrete score (1-5) for the metric.
Step 3: Converts it to a metric between 0 and 1. 

<b>Calculation (include input, calculation/ formula, output details)</b>
Prompts are crafted for this criteria, taking the original document and the summary as inputs, and leveraging chain-of-thought generation and guiding the model to output a numeric score from 1-5 for the criteria.
Scores are generated from gpt-4 with the defined prompts, comparing them across summaries.

<b>Output:</b> Is a value between 0 and 1. The higher the value, more coherent the summary is to the document.                 
      </div>

      <div id="matrSum_fluency">
        <h2 class="pageTitle">Robustness</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_coherance')">Coherence</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_fluency')">Fluency</button>
        </div>
(This is an LLM Based Evaluator)

Rates the grammar and readability of the summary.

<b>Calculation (include input, calculation/ formula, output details)</b>
  Prompts are crafted for this criteria, taking the original document and the summary as inputs, and leveraging chain-of-thought generation and guiding the model to output a numeric score from 1-5 for the criteria.
  
  Scores are generated from gpt-4 / any LLM with the defined prompts, comparing them across summaries.

<b>Required Arguments: (your dataset must contain these fields)</b>
  <b>original text/ input:</b> The source document that contains the information that should be summarized.
  <b>actual output:</b> The LLM generated summary of the source document.
  
  Step 1: Uses prompts to instruct an LLM to score the summary on relevancy. 
  Step 2: Uses a direct scoring function where gpt-4/ any LLM generates a discrete score (1-5) for the metric.
  Step 3: Converts it to a metric between 0 and 1. 

<b>Output: </b>
  Is a value between 0 and 1. The higher the value, more relevant the summary to the document. 
      </div>

      <div id="matrSum_cost">
        <h2 class="pageTitle">Efficiency</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrSum_cost')">Cost</button>
          <button class="button button2" onClick="clickCalled('matrSum_latency')">Latency</button>
        </div>
Measures the total for a GenAI application to generate a response from the moment it receives an input until the output is provided. This also includes the cost of evaluation of the response. 

<b>Required Arguments:</b>
number of tokens for response, token cost

<b>Calculation: </b>
This metric is calculated in the backend. Consider calculating the number of tokens for a response. Can be calculated using call back function. 

<b>Cost of LLM inference</b> = (Number of input tokens * Rate of LLM for inputs) + (Number of out tokens * Rate of LLM for output)

<b>Output</b> is the cost of the application in dollars.        
      </div>

      <div id="matrSum_latency">
        <h2 class="pageTitle">Efficiency</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_cost')">Cost</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_latency')">Latency</button>
        </div>
Logs the time taken for a GenAI application to generate a response from the moment it receives an input until the output is provided.

Helps assess the responsiveness and real-time performance of the application. This is an important aspect of GenAI applications that generate content in real-time. Latency allows the user to track a GenAI application and compare it with other applications or models. It captures the model's performance and responsiveness under different loads/ scenarios. 

<b>Required Arguments:</b>
the start time from the application and end time from the application to process a user query

<b>Calculation: </b>
    Latency = Time at output - Time at Input

Output is the time in Milliseconds (ms)/ Seconds (s).
      </div>

      <div id="matrSum_gender">
        <h2 class="pageTitle">Bias</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrSum_gender')">Gender and Racial Bias</button>
          <button class="button button2" onClick="clickCalled('matrSum_honest')">Honest Score</button>
        </div>
(This is an LLM Based Evaluator)

This metric determines whether there is gender or racial bias in the GenAI application / LLM response. 

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>input:</b> The user query that is input to the application. 
<b>response:</b> actual output generated by the GenAI application/ LLM.

<b>Calculation :</b>

The BiasMetric is calculated according to the following equation: 

Bias = Number of biased opinions
    Total number of opinions
  
Step 1: Use an LLM all the opinions found in the response.
Step 2: Use an LLM to classify each opinion as biased or not.

<b>Definition of Bias :</b>
  • Gender Bias: Discrimination based on a person's gender. 
  • Racial/ Ethnic Bias: Prejudice or discrimination based on a person's race, ethnicity or national origin. 
  • Geographical Bias: Prejudice or preferential treatment based on where a person lives or comes from. 

<b>Definition of Opinion:</b>
  • Opinions are personal beliefs or judgements, not verifiable facts. 
  • A mistaken statement
  • Citation of a source, a reported statement, not a subjective opinion.

<b>Output: </b>
  The percentage of biased opinions in the response. A number between 0 and 1. 
      </div>

      <div id="matrSum_honest">
        <h2 class="pageTitle">Bias</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_gender')">Gender and Racial Bias</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_honest')">Honest Score</button>
        </div>
(This is an LLM Based Evaluator)

• The HONEST score aims to measure hurtful sentence completions in language models. This score uses HurtLex, a multilingual lexicon of hurtful language, to evaluate the completions. Its goal is to quantify how many times a sentence is completed with a hurtful word and if there is a difference between groups (e.g. genders, sexual orientations, etc.).

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>input:</b> The user query that is input to the application. 
<b>response:</b> The actual output generated by the GenAI application/ LLM.

<b>Calculation :</b>

  •  This is a text-based metric, make sure to wrap you input in double quotes.

<b>Output: </b>
  The percentage of . A number between 0 and 1. 
      </div>

      <div id="matrSum_toxic">
        <h2 class="pageTitle">Toxicity</h2>
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrSum_toxic')">Toxic Opinions</button>
          <button class="button button2" onClick="clickCalled('matrSum_abusive')">Abusive Speech</button>
        </div>
(This is an LLM Based Evaluator)

This metric determines whether there is toxicity in the GenAI application / LLM response. 

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>input:</b> The user query that is input to the application. 
<b>response:</b> The actual output generated by the GenAI application/ LLM.

<b>Calculation :</b>

The ToxicityMetric is calculated according to the following equation: 

    Toxicity = Number of toxic opinions / Total number of opinions
  
Step 1: Use an LLM all the opinions found in the response.
Step 2: Use an LLM to classify each opinion as toxic or not.

<b>Definition of Toxic Opinion:</b>
  • <b>Personal Attacks:</b> Insults or hostile comments aimed at degrading the individual rather than addressing their ideas.
  • <b>Mockery:</b> Sarcasm or ridicule used to belittle someone.
  • <b>Hate:</b> Expressions of intense dislike or disgust, often targeting someone's identity or beliefs.
  • <b>Dismissive Statements:</b> Comments that invalidate the person's viewpoint or shut down discussion without engaging constructively.
  • <b>Threats or Intimidation:</b> Statements intending to frighten, control, or harm someone, either physically or emotionally.
  
<b>Output: </b>
  The percentage of toxic opinions in the response. A number between 0 and 1.     
      </div>

      <div id="matrSum_abusive">
        <h2 class="pageTitle">Toxicity</h2>
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrSum_toxic')">Toxic Opinions</button>
          <button class="button button2 active" onClick="clickCalled('matrSum_abusive')">Abusive Speech</button>
        </div>
(This is an LLM based evaluator)
The toxicity measurement aims to quantify the toxicity of the input texts using a pretrained hate speech classification model. The default model used is roberta-hate-speech-dynabench-r4. In this model, ‘hate’ is defined as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” 

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>Input Text:</b> The actual output generated by the GenAI application/ LLM. Is a collection of sentences.

<b>Calculation: </b>
Returns the maximum toxicity value computed for the sentences in the input text. We can also choose to compute the ratio of toxic sentences in the input text. 

<b>Output: </b>
Returns the maximum toxicity over all the scores on the input text.  

If `dataset_flag` is False, a single float value is returned. 

If `dataset_flag` is True, a list of dictionaries containing the response and corresponding score is returned. 
      </div>

      <div id="contModelBench">
        <h2 class="pageTitle">Model Benchmarking</h2>
1. Model Evaluation results on the SME validated datasets.
2. AWS model evaluation results
3. VertexAI model evaluation
      </div>

      <div id="contMethodSummarization">
        <h2 class="pageTitle">Methods for Summarization</h2>
To evaluate the quality, consistency and reasoning ability of the Language Model with different prompting techniques for generating a summary. 

<b>Direct Prompting:</b> Simple, explicit prompt to model to generate a summary. 

<b>Chain of Thought Prompting:</b> involves generating a summary by guiding a LM through a structured 
sequential process with stepwise instructions to the model. 

<b>Self Consistency Prompting:</b> tracks the percentage of times the application gives the same answer across 
multiple runs for the same input document. 

<b>Meta prompting</b> involves instructions with example based guidance. A language model reflects on its 
reasoning to generate a summary. 

<b>Adversarial Prompting:</b> intentionally introduces errors or challenges to test the robustness of summarization. 
        
      </div>

    </div>
    <!-- JavaScript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="script.js"></script>
  </body>
</html>
